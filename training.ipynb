{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras import regularizers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a new model\n",
    "- Load the features/targets\n",
    "- Create a model\n",
    "- Save it with a nice name to 'models/{}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 100,000 rows of random vs random (gen 0) data\n",
    "key = 'random-2017-10-21-13:41:47'\n",
    "\n",
    "# 10,000 rows of gen-1 vs gen-1 data\n",
    "key = 'gen-1-cov2d_beta_2017_10_22_142925'\n",
    "\n",
    "# 5000 rows of covnet vs covnet (gen 1) data\n",
    "#key = 'gen-1-2017-10-22-133109'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = pd.read_csv('training_data/{}/features.csv'.format(key),\n",
    "                       names=['game_idx', 'turn_idx'] + range(42),\n",
    "                       dtype='int')\n",
    "\n",
    "targets = pd.read_csv('training_data/{}/targets.csv'.format(key),\n",
    "                      names=['game_idx', 'turn_idx', 'win', 'lose', 'draw'],\n",
    "                      dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into training and holdout,\n",
    "# ensuring separate games \n",
    "\n",
    "train_games = pd.Series(features.game_idx.unique()).sample(frac=0.8).values #games = #features.game_idx.unique()\n",
    "\n",
    "features_train = features[features.game_idx.isin(train_games)].drop(['game_idx', 'turn_idx'], axis=1)\n",
    "targets_train = targets[targets.game_idx.isin(train_games)].drop(['game_idx', 'turn_idx'], axis=1)\n",
    "\n",
    "tidx = list(features_train.index)\n",
    "random.shuffle(tidx)\n",
    "features_train = features_train.loc[tidx]\n",
    "targets_train = targets_train.loc[tidx]\n",
    "\n",
    "features_test = features[~features.game_idx.isin(train_games)].drop(['game_idx', 'turn_idx'], axis=1)\n",
    "targets_test = targets[~targets.game_idx.isin(train_games)].drop(['game_idx', 'turn_idx'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "145558 116499 29059\n",
      "145558 116499 29059\n",
      "1    0.518728\n",
      "0    0.481272\n",
      "Name: win, dtype: float64\n",
      "0    0.522383\n",
      "1    0.477617\n",
      "Name: lose, dtype: float64\n",
      "0    0.996345\n",
      "1    0.003655\n",
      "Name: draw, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print len(features), len(features_train), len(features_test)\n",
    "print len(targets), len(targets_train), len(targets_test)\n",
    "\n",
    "print targets.win.value_counts(normalize=True)\n",
    "print targets.lose.value_counts(normalize=True)\n",
    "print targets.draw.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch_iter(batch_size, batch_idx, dfs):\n",
    "    \n",
    "    length = len(dfs[0])\n",
    "    for df in dfs:\n",
    "        assert len(df) == length\n",
    "        \n",
    "    batches_per_df = int(math.ceil(length / batch_size))\n",
    "    \n",
    "    local_idx = batch_idx % batches_per_df\n",
    "    \n",
    "    start = local_idx*batch_size\n",
    "    end = (local_idx+1)*batch_size\n",
    "    \n",
    "    return [df.iloc[start:end] for df in dfs]\n",
    "\n",
    "\n",
    "def get_batch_random(batch_size, _, dfs):    \n",
    "    mask = pd.Series(dfs[0].index).sample(n=batch_size, replace=False)    \n",
    "    return [df.loc[mask] for df in dfs]\n",
    "\n",
    "def get_batch(batch_size, batch_idx, dfs, how='iter'):\n",
    "    if how == 'iter':\n",
    "        return get_batch_iter(batch_size, batch_idx, dfs)\n",
    "    elif how == 'random':\n",
    "        return get_batch_random(batch_size, batch_idx, dfs)\n",
    "    else:\n",
    "        raise Exception()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(graph, output_prefix, batch_size, num_batches, batch_how='iter'):\n",
    "    \n",
    "    with tf.Session(graph=graph) as sess:\n",
    "    \n",
    "        K.set_session(sess)\n",
    "        sess.run(init_op)\n",
    "    \n",
    "        #suffix = datetime.datetime.now().strftime(\"%Y_%m_%d_%H:%M:%S\")\n",
    "        #name = output_prefix #'{}_{}'.format(output_prefix, suffix)\n",
    "        #save_name = \"{}/model\".format(name) #, suffix)\n",
    "\n",
    "        train_writer = tf.summary.FileWriter(output_prefix, sess.graph)\n",
    "        \n",
    "        print \"Running {}\".format(output_prefix)\n",
    "            \n",
    "        t = time.time()\n",
    "        delta_t = 0\n",
    "        \n",
    "        for i in range(num_batches):\n",
    "        \n",
    "            if i % 1000 == 0:\n",
    "                \n",
    "                delta_t = time.time() - t\n",
    "                t = time.time()\n",
    "                    \n",
    "                acc = acc_value.eval(feed_dict={board: features_test, outcome: targets_test}).mean()\n",
    "                print \"Batch {} Hold-Out Accuracy: {} Time taken: {:.1f}s\".format(i, acc, delta_t)\n",
    "\n",
    "                summary = sess.run(all_summaries, feed_dict={board: features_test, outcome: targets_test})\n",
    "                train_writer.add_summary(summary, i)            \n",
    "            \n",
    "            batch = get_batch(batch_size, i, [features_train, targets_train], how=batch_how)        \n",
    "            train_step.run(feed_dict={board: batch[0], outcome: batch[1]})\n",
    "        \n",
    "        print \"DONE TRAINING\"\n",
    "        print \"FINAL ACCURACY: {}\".format(acc)\n",
    "        train_writer.close()\n",
    "        \n",
    "        model_dir = '{}/model'.format(output_prefix)\n",
    "        print \"SAVING TO: {}\".format(model_dir)\n",
    "        tf.train.Saver().save(sess, model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 1: Normal dense fully-connected neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.objectives import categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy as accuracy\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Create a graph to hold the model.\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Create model in the graph.\n",
    "with graph.as_default():\n",
    "\n",
    "    # Keras layers can be called on TensorFlow tensors:\n",
    "    board = tf.placeholder(tf.float32, shape=(None, 42), name='board')\n",
    "    outcome = tf.placeholder(tf.float32, shape=(None, 3), name='outcome')\n",
    "\n",
    "    # Fully connected layers\n",
    "    \n",
    "    x = Dense(512,\n",
    "              activation='tanh',\n",
    "              kernel_regularizer=regularizers.l2(0.1),\n",
    "              bias_regularizer=regularizers.l2(0.1),\n",
    "              kernel_initializer='random_uniform',\n",
    "              bias_initializer='zeros')(board)\n",
    "    \n",
    "    x = Dense(512,\n",
    "              activation='tanh',\n",
    "              kernel_regularizer=regularizers.l2(0.1),\n",
    "              bias_regularizer=regularizers.l2(0.1),\n",
    "              kernel_initializer='random_uniform',\n",
    "              bias_initializer='zeros')(x)\n",
    "   \n",
    "    x = Dense(512,\n",
    "              activation='tanh',\n",
    "              kernel_regularizer=regularizers.l2(0.1),\n",
    "              bias_regularizer=regularizers.l2(0.1),\n",
    "              kernel_initializer='random_uniform',\n",
    "              bias_initializer='zeros')(x)\n",
    "    \n",
    "    x = Dense(48,\n",
    "              activation=activation,\n",
    "              kernel_regularizer=regularizers.l2(0.1),\n",
    "              bias_regularizer=regularizers.l2(0.1),\n",
    "              kernel_initializer='random_uniform',\n",
    "              bias_initializer='zeros')(x)\n",
    "\n",
    "    # output layer with 10 units and a softmax activation\n",
    "    preds = Dense(3, activation='softmax', name='preds')(x) \n",
    "        \n",
    "    with tf.name_scope('evaluation') as scope:\n",
    "        loss = tf.reduce_mean(categorical_crossentropy(outcome, preds))\n",
    "        tf.summary.scalar('loss', loss)    \n",
    "    \n",
    "        acc_value = accuracy(outcome, preds)\n",
    "        tf.summary.scalar('accuracy', tf.reduce_mean(acc_value))\n",
    "    \n",
    "    with tf.name_scope('training') as scope:\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)\n",
    "    \n",
    "    # Initialize all variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ./models/dense_2017_10_21_16:54:47\n",
      "Batch 0 Hold-Out Accuracy: 0.340285211802 Time taken: 0.0s\n",
      "Batch 1000 Hold-Out Accuracy: 0.573109030724 Time taken: 30.2s\n",
      "Batch 2000 Hold-Out Accuracy: 0.571829020977 Time taken: 28.7s\n",
      "Batch 3000 Hold-Out Accuracy: 0.572524368763 Time taken: 28.2s\n",
      "Batch 4000 Hold-Out Accuracy: 0.576720535755 Time taken: 28.1s\n",
      "Batch 5000 Hold-Out Accuracy: 0.575209498405 Time taken: 29.6s\n",
      "Batch 6000 Hold-Out Accuracy: 0.576876878738 Time taken: 31.1s\n",
      "Batch 7000 Hold-Out Accuracy: 0.570325255394 Time taken: 30.6s\n",
      "Batch 8000 Hold-Out Accuracy: 0.576917827129 Time taken: 28.1s\n",
      "Batch 9000 Hold-Out Accuracy: 0.577545762062 Time taken: 27.6s\n",
      "DONE TRAINING\n",
      "FINAL ACCURACY: 0.577545762062\n",
      "SAVING TO: ./models/dense_2017_10_21_16:54:47/model\n"
     ]
    }
   ],
   "source": [
    "run(graph, './models/dense_{}'.format(datetime.datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")),\n",
    "    batch_size=200,\n",
    "    num_batches=10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 2: A number of parallel CovNets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "# Create a graph to hold the model.\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Create model in the graph.\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Keras layers can be called on TensorFlow tensors:\n",
    "    board = tf.placeholder(tf.float32, shape=(None, 42), name='board') \n",
    "    outcome = tf.placeholder(tf.float32, shape=(None, 3), name='outcome')    \n",
    "    \n",
    "    dense_args = dict(\n",
    "        use_bias=True,\n",
    "        activation='relu',\n",
    "        kernel_initializer='random_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=regularizers.l2(0.01),\n",
    "        bias_regularizer=regularizers.l2(0.01),    \n",
    "    )\n",
    "    \n",
    "    d1 = Dense(512, **dense_args)(board) #activation='relu')(board) \n",
    "        \n",
    "    # The input data is [col0=[row_0, row_1, ...], col1=[row_0, row_1], ...]\n",
    "    rs = Reshape((7, 6, 1), input_shape=(42,))(board)\n",
    "    \n",
    "    conv_args = dict(\n",
    "        use_bias=True,\n",
    "        activation='relu',\n",
    "        kernel_initializer='random_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=regularizers.l2(0.01),\n",
    "        input_shape=(7, 6, 1),\n",
    "        padding='valid'\n",
    "    )\n",
    "    \n",
    "    c1 = Flatten()(Conv2D(1, kernel_size=(4, 4), **conv_args)(rs)) #use_bias=True, activation='relu', input_shape=(7, 6, 1))(rs))\n",
    "    c2 = Flatten()(Conv2D(1, kernel_size=(1, 4), **conv_args)(rs)) #use_bias=True, activation='relu', input_shape=(7, 6, 1))(rs))\n",
    "    c3 = Flatten()(Conv2D(1, kernel_size=(4, 1), **conv_args)(rs)) #use_bias=True, activation='relu', input_shape=(7, 6, 1))(rs))\n",
    "    c4 = Flatten()(Conv2D(1, kernel_size=(5, 5), **conv_args)(rs)) #use_bias=True, activation='relu', input_shape=(7, 6, 1))(rs))\n",
    "    c5 = Flatten()(Conv2D(1, kernel_size=(3, 3), **conv_args)(rs)) #use_bias=True, activation='relu', input_shape=(7, 6, 1))(rs))    \n",
    "    c6 = Flatten()(Conv2D(1, kernel_size=(2, 2), **conv_args)(rs)) #use_bias=True, activation='relu', input_shape=(7, 6, 1))(rs))\n",
    "\n",
    "    merged = Concatenate()([d1, c1, c2, c3, c4, c5, c6])\n",
    "    \n",
    "    x = Dense(256,  **dense_args)(merged)         \n",
    "    x = Dense(128,  **dense_args)(x)    \n",
    "    x = Dense(12,   **dense_args)(x)\n",
    "    \n",
    "    # output layer with 10 units and a softmax activation\n",
    "    preds = Dense(3, activation='softmax', name='preds')(x) \n",
    "    \n",
    "    with tf.name_scope('evaluation') as scope:\n",
    "        loss = tf.reduce_mean(categorical_crossentropy(outcome, preds))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "        acc_value = accuracy(outcome, preds)\n",
    "        tf.summary.scalar('accuracy', tf.reduce_mean(acc_value))\n",
    "    \n",
    "    with tf.name_scope('training') as scope:\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate=0.0001).minimize(loss)    \n",
    "    \n",
    "    # Initialize all variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ./models/cov2d_2017_10_22_11:01:23\n",
      "Batch 0 Hold-Out Accuracy: 0.481537163258 Time taken: 0.0s\n",
      "Batch 1000 Hold-Out Accuracy: 0.574054598808 Time taken: 29.3s\n",
      "Batch 2000 Hold-Out Accuracy: 0.58172750473 Time taken: 33.1s\n",
      "Batch 3000 Hold-Out Accuracy: 0.588079452515 Time taken: 34.4s\n",
      "Batch 4000 Hold-Out Accuracy: 0.590709269047 Time taken: 33.0s\n",
      "Batch 5000 Hold-Out Accuracy: 0.590805530548 Time taken: 33.4s\n",
      "Batch 6000 Hold-Out Accuracy: 0.591366112232 Time taken: 36.8s\n",
      "Batch 7000 Hold-Out Accuracy: 0.593473851681 Time taken: 37.9s\n",
      "Batch 8000 Hold-Out Accuracy: 0.593411266804 Time taken: 37.7s\n",
      "Batch 9000 Hold-Out Accuracy: 0.593921363354 Time taken: 38.2s\n",
      "Batch 10000 Hold-Out Accuracy: 0.591291546822 Time taken: 33.5s\n",
      "Batch 11000 Hold-Out Accuracy: 0.593464195728 Time taken: 37.7s\n",
      "Batch 12000 Hold-Out Accuracy: 0.591301143169 Time taken: 38.2s\n",
      "Batch 13000 Hold-Out Accuracy: 0.594157159328 Time taken: 37.1s\n",
      "Batch 14000 Hold-Out Accuracy: 0.595247089863 Time taken: 35.6s\n",
      "Batch 15000 Hold-Out Accuracy: 0.59478032589 Time taken: 39.4s\n",
      "Batch 16000 Hold-Out Accuracy: 0.594166755676 Time taken: 44.8s\n",
      "Batch 17000 Hold-Out Accuracy: 0.595851004124 Time taken: 43.9s\n",
      "Batch 18000 Hold-Out Accuracy: 0.595940053463 Time taken: 39.4s\n",
      "Batch 19000 Hold-Out Accuracy: 0.594530105591 Time taken: 44.1s\n",
      "DONE TRAINING\n",
      "FINAL ACCURACY: 0.594530105591\n",
      "SAVING TO: ./models/cov2d_2017_10_22_11:01:23/model\n"
     ]
    }
   ],
   "source": [
    "sess = run(graph,\n",
    "           './models/cov2d_{}'.format(datetime.datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")),\n",
    "           batch_size=500,\n",
    "           num_batches=20000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attempt 3: More complicated covnets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D, Flatten\n",
    "from keras.layers.core import Reshape\n",
    "from keras.layers.merge import Concatenate\n",
    "\n",
    "# Create a graph to hold the model.\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Create model in the graph.\n",
    "with graph.as_default():\n",
    "    \n",
    "    # Keras layers can be called on TensorFlow tensors:\n",
    "    board = tf.placeholder(tf.float32, shape=(None, 42), name='board') \n",
    "    outcome = tf.placeholder(tf.float32, shape=(None, 3), name='outcome')    \n",
    "    \n",
    "    dense_args = dict(\n",
    "        use_bias=True,\n",
    "        activation='relu',\n",
    "        kernel_initializer='random_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=regularizers.l2(0.01),\n",
    "        bias_regularizer=regularizers.l2(0.01),    \n",
    "    )\n",
    "    \n",
    "    d1 = Dense(512, **dense_args)(board) #activation='relu')(board) \n",
    "        \n",
    "    # The input data is [col0=[row_0, row_1, ...], col1=[row_0, row_1], ...]\n",
    "    rs = Reshape((7, 6, 1), input_shape=(42,))(board)\n",
    "    \n",
    "    conv_args = dict(\n",
    "        use_bias=True,\n",
    "        activation='relu',\n",
    "        kernel_initializer='random_uniform',\n",
    "        bias_initializer='zeros',\n",
    "        kernel_regularizer=regularizers.l2(0.01),\n",
    "        input_shape=(7, 6, 1),\n",
    "        padding='valid'\n",
    "    )\n",
    "    \n",
    "    # We use a few parallel covents, that we combine in the end        \n",
    "    ca = Flatten()(Conv2D(8, kernel_size=(5, 5), **conv_args)(rs))\n",
    "    #ca2 = Dense(3, **dense_args)(ca1)\n",
    "\n",
    "    cb = Flatten()(Conv2D(8, kernel_size=(4, 4), **conv_args)(rs))\n",
    "    #cb2 = Dense(3, **dense_args)(cb1)\n",
    "\n",
    "    cc = Flatten()(Conv2D(4, kernel_size=(1, 4), **conv_args)(rs))\n",
    "    #cc2 = Dense(3, **dense_args)(cc1)\n",
    "    \n",
    "    cd = Flatten()(Conv2D(4, kernel_size=(4, 1), **conv_args)(rs))\n",
    "    #cd2 = Dense(3, **dense_args)(cd1)\n",
    "\n",
    "    merged = Concatenate()([d1, ca, cb, cc, cd])\n",
    "    \n",
    "    x = Dense(256,  **dense_args)(merged)         \n",
    "    x = Dense(128,  **dense_args)(x)    \n",
    "    x = Dense(12,   **dense_args)(x)\n",
    "    \n",
    "    # output layer with 10 units and a softmax activation\n",
    "    preds = Dense(3, activation='softmax', name='preds')(x) \n",
    "    \n",
    "    with tf.name_scope('evaluation') as scope:\n",
    "        loss = tf.reduce_mean(categorical_crossentropy(outcome, preds))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "        \n",
    "        acc_value = accuracy(outcome, preds)\n",
    "        tf.summary.scalar('accuracy', tf.reduce_mean(acc_value))\n",
    "    \n",
    "    with tf.name_scope('training') as scope:\n",
    "        train_step = tf.train.AdamOptimizer(learning_rate=0.01).minimize(loss)    \n",
    "    \n",
    "    # Initialize all variables\n",
    "    init_op = tf.global_variables_initializer()\n",
    "    \n",
    "    all_summaries = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running ./models/gen1-cov2d_beta_2017_10_22_150410\n",
      "Batch 0 Hold-Out Accuracy: 0.268591493368 Time taken: 0.0s\n",
      "Batch 1000 Hold-Out Accuracy: 0.64998793602 Time taken: 19.7s\n",
      "Batch 2000 Hold-Out Accuracy: 0.656113445759 Time taken: 21.8s\n",
      "Batch 3000 Hold-Out Accuracy: 0.656147837639 Time taken: 22.8s\n",
      "Batch 4000 Hold-Out Accuracy: 0.651226818562 Time taken: 25.7s\n",
      "Batch 5000 Hold-Out Accuracy: 0.65576928854 Time taken: 25.4s\n",
      "Batch 6000 Hold-Out Accuracy: 0.650194406509 Time taken: 23.7s\n",
      "Batch 7000 Hold-Out Accuracy: 0.648577034473 Time taken: 25.7s\n",
      "Batch 8000 Hold-Out Accuracy: 0.654151916504 Time taken: 23.5s\n",
      "Batch 9000 Hold-Out Accuracy: 0.652740955353 Time taken: 23.9s\n",
      "Batch 10000 Hold-Out Accuracy: 0.650848269463 Time taken: 24.9s\n",
      "Batch 11000 Hold-Out Accuracy: 0.648955583572 Time taken: 27.1s\n",
      "Batch 12000 Hold-Out Accuracy: 0.650882661343 Time taken: 24.5s\n",
      "Batch 13000 Hold-Out Accuracy: 0.65370452404 Time taken: 25.0s\n"
     ]
    }
   ],
   "source": [
    "sess = run(graph,\n",
    "           './models/gen1-cov2d_beta_{}'.format(datetime.datetime.now().strftime(\"%Y_%m_%d_%H%M%S\")),\n",
    "           batch_size=500,\n",
    "           num_batches=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "connectfour",
   "language": "python",
   "name": "connectfour"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
